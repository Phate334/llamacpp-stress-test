# LlamaCPP Stress Test Configuration Example
# This is a sample configuration file showing available options

# Test duration in seconds
test_duration=60

# Maximum number of concurrent batches to test
max_concurrent_batches=32

# Memory limit in MB (for resource monitoring)
max_memory_mb=4096

# Output configuration
output_format=json
output_file=stress_test_results.json

# Model configuration (optional - for llama.cpp integration)
# model_path=/path/to/your/model.gguf
# context_size=2048
# gpu_layers=0
# threads=4

# Logging
verbose=false